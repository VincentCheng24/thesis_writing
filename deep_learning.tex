\subsection{Deep Learning Technology}

Recently, Deep Learning (DL) has shown its impressive power in a variety of tasks, especially in some complex tasks that can not be explicitly programmed by hand, such as anomaly detection and online advertising. DL delivers the solutions by learning from data automatically via a general learning procedure which dominantly makes use of backpropagation and optimization algorithms, \eg gradient descent.

It attracts the world-wide attention mainly by outperforming other classic machine learning algorithms, \eg Support Vector Machine (SVM),  in many competitions  involved with image classification, object detection, or nature language processing. This is because DL applys a deep automatic feature learning architecture, \ie a artificial neural network model with multiple hidden layers, to learn deep distributed hierarchical nonlinear representations which yield better performance for learning tasks, \eg in terms of classification accuracy. Such representations bring many good properties, such as feature reuse, parameter sharing,  multiple levels of progressive abstraction, and invariance to local changes of the raw inputs, and thus provide better predictive power than classic machine learning algorithms \cite{6472238}.

Convolutional neural networks (CNNs) is one of the major branches of artificial neural networks. A CNN is a feed-forward multi-layer neural network, typically consisting of one or more convolutional layers,  interleaved by some pooling layers, and finally followed by some fully-connected layers. This modern framework of CNNs is established by LeCun \etal when he proposed the handwritten digit classifier LeNet-5 \cite{726791}. Afterwards, more and more deeper architectures are explored such as AlexNet \cite{DBLP:Russakovsky14}, VGGNet \cite{DBLP:SimonyanZ14a}, GoogleNet \cite{DBLP:SzegedyLJSRAEVR14} and ResNet \cite{DBLP:HeZRS15}. In general, deeper architectures, bring better feature representations, and closer approximations to the target function, as well as more complex models which are more difficult to train and easier to be overfitting. Therefore many approaches have been proposed to address such problems.

CNNs are specifically designed to work with problems taking images as inputs, such as image classification, object detection and pose estimation. The above-mentioned CNNs examples all made great performance in their respective tasks, \eg ResNet won the first prize in ILSVRC 2015.

The core building block, convolutional layers, is used to compute feature maps with convolution kernels. Each neuron in a convolutional layer is connected to a local region in the previous layer, called receptive field, and computes an output by performing an convolution operation (element-wise matrix multiplication) between its weights (kernel) and the connected region followed by an non-linear activation function. A nice property of CNNs is that the kernel  is shared by all receptive fields in the preceding layer when computing the corresponding feature map. Thus, each feature map is used to capture exactly the same feature but at different locations. And this characteristic can substantially reduce the number of parameters in CNNs, which will lead to faster training \cite{nndd}.  

The pooling layers perform a downsampling operation, typically max pooling \cite{Boureau:2010} and average pooling \cite{6460871}, along the spatial dimensions of feature maps to achieve shift-invariance. 

Normally, successive convolution layers detect more abstract features, \eg wheels, than the preceding ones that tend to detect low-level features, \eg curves and edges. Therefore, after the operations performed by several convolutional and pooling layers, progressively higher-level features can be obtained to feed a fully-connected layer whose goal is to perform high-level reasoning to generate the global semantic information \cite{DBLP:SimonyanZ14a, DBLP:abs-1207-0580}.

For classification problems, the output layer of CNNs usually deploys the softmax operator \cite{DBLP:Russakovsky14} or the SVM method \cite{DBLP:Tang13} to output discrete results. While regression tasks require continuous-valued predictions so that the output layer should have a linear activation function, \eg weighted sum, along with a proper cost function, \eg mean squared error \cite{DBLP:ZhouHSZ16}.

The training for CNNs is a global optimization problem by minimizing the defined loss function. Normally, CNNs can be trained end-to-end efficiently with backpropagation together with an optimization algorithm, such as stochastic gradient descent \cite{5597822}. The mechanism behind it is that gradient of the loss function w.r.t. all parameters is calculated and then used to update the parameters to the direction of minimizing the loss function based on iterations over the full batch or mini batches of the training dataset.














