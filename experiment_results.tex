\subsection{Experiment Results}
\label{exp_res}
In this subsection, we present the best results of six tasks achieved by our approach and compare them with the state-of-the-art works in monocular 3D vehicle detection.

\subsubsection{3D Bounding Box Estimation} 
3D vehicle detection refers to the 3D bounding box estimation of the target vehicles in images. A 3D bounding box is estimated correctly, if $IoU>0.7$. Our approach is based on 2D object detection, so in order to be comparable with other methods, we eliminate the influence of this foundation by multiplying our results with the top accuracy of 2D car detection in KITTI achieved by iDST-VC \cite{2dobject}. Our results and the comparison are shown in Table \ref{3D_vehicle_detection}. A LiDAR-based method, VoxelNet++ \cite{DBLP:journals/corr/abs-1711-06396}, achieves the best performance currently. Based on the assumption above, our method currently ranks at the middle ($24^{th}$) on 3D vehicle detection benchmark \cite{3dobject}. It can achieve similar performance as VoxelNet basic \cite{DBLP:journals/corr/abs-1711-06396}. It outperforms all traceable image-based methods and some of LiDAR-based methods. For more comparison, check \cite{3dobject}. Besides, our method performs 3D detection for all cars and vans on KITTI dataset while the results from other methods are for cars only. Therefore, theoretically, the performance of our approach on cars is at least not worse than on both cars and vans. 

\tbd [evaluation, mine x 2D acc.  because hard to get the numerical data from graph with lowest recall]

\begin{table}[H]
	\centering
	\caption{Comparison of the 3D bounding box estimation on official KITTI dataset for cars (ours is for cars and vans)}
	\label{3D_vehicle_detection}
	\begin{tabular}{|c|c|c|c|c|}
		\hline
		Method                                                                   & Type  & Easy           & Moderate       & Hard           \\ \hline
		3D-SSMFCNN \cite{novakmaster2017}                       & Mono  & 2.39           & 2.28           & 1.52           \\ \hline
		A3DODWTDA \cite{erino397fregu856master2018}             & Mono  & 6.76           & 6.45           & 4.87           \\ \hline
		DoBEM \cite{8088147}                                    & LiDAR & 7.42           & 6.95           & 13.45          \\ \hline
		LMNetV2 \cite{2018arXiv180504902M}                      & LiDAR & 14.75          & 15.24          & 12.85          \\ \hline
		VoxelNet basic \cite{DBLP:journals/corr/abs-1711-06396} & LiDAR & 29.70          & 24.35          & 23.52          \\ \hline
		VoxelNet++ \cite{DBLP:journals/corr/abs-1711-06396}     & LiDAR & \textbf{83.13} & \textbf{73.66} & \textbf{66.20} \\ \hline
		Ours (ResNet50)                                                          & Mono  & 30.90          & 24.16          & 18.53          \\ \hline
		Ours (VGG19)																& Mono	& 30.36	& 23.92	& 20.11 \\ \hline
		
	\end{tabular}
\end{table}


\subsubsection{3D Localization}
3D localization refers to the estimation of the location of the vehicle's center. We follow the evaluation metric used by \cite{DBLP:journals/corr/MousavianAFK16, DBLP:journals/corr/ChabotCRTC17} that a 3D localization is correct if the distance between the true and predicted center is smaller than a threshold and 1 meter and 2 meters are used as thresholds. And we borrow the idea from \cite{DBLP:journals/corr/MousavianAFK16} to calculate the 3D Localization accuracy,which is the ratio between Average Localization Precision (ALP) and Average Precision (AP), for other methods. We present our results and the comparison in Table \ref{3d_local}. Clearly, our method outperforms all other monocular methods, while can't match the perform of stereo 3DOP \cite{Chen:2015:OPA:2969239.2969287}.

\tbd [3DL = ALP / AP]
\tbd [3DOP is very high, include it ?]

\begin{table}[H]
	\centering
	\caption{Comparison of the 3D Localization accuracy on official KITTI dataset for cars (ours is for cars and vans)}
	\label{3d_local}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|c|c|c|c|c|c|c|c|}
			\hline
			\multirow{2}{*}{Method}                                      & \multirow{2}{*}{Type} & \multicolumn{3}{c|}{Loc. \textless 1m}           & \multicolumn{3}{c|}{Loc. \textless 2m}           \\ \cline{3-8} 
			&                       & Easy           & Moderate       & Hard           & Easy           & Moderate       & Hard           \\ \hline
			3DOP \cite{Chen:2015:OPA:2969239.2969287}   & Stereo                & \textbf{83.69} & \textbf{65.96} & \textbf{65.71} & \textbf{98.57} & \textbf{88.08} & \textbf{91.56} \\ \hline
			DPM \cite{5255236}                          & Mono                  & 34.33          & 29.02          & 29.18          & 56.48          & 46.69          & 46.17          \\ \hline
			3DVP \cite{xiang2015data}                   & Mono                  & 52.15          & 45.24          & 42.40          & 76.10          & 68.00          & 64.84          \\ \hline
			SubCNN \cite{DBLP:journals/corr/XiangCLS16} & Mono                  & 43.26          & 34.86          & 32.75          & 77.66          & 63.12          & 59.33          \\ \hline
			Ours (ResNet50)                                              & Mono                  & 67.87          & 59.96          & 51.38          & 87.56                & 79.83                & 73.11                \\ \hline
			Ours (VGG19)	                                             & Mono	                  & 68.25	          & 56.58	          & 54.53		          & 88.39	          & 78.63	          & 77.49                 \\ \hline
			
		\end{tabular}%
	}
\end{table}

\subsubsection{3D Orientation Estimation}
3D Orientation Estimation refers to the estimation of rotation of vehicle along the Y-axis (yaw axis), $r_y$, in the camera coordinate systems. The evaluation metric is Orientation Score (OS), which is the ratio between Average Orientation Estimation (AOS) and Average Precision (AP) \cite{DBLP:journals/corr/MousavianAFK16}. Table \ref{os_cmp} shows the results of our methods and other state-of-the-art ones. Although our method can't achieve the better performance than , the performance than SubCNN \cite{DBLP:journals/corr/XiangCLS16},  Deep3DBox \cite{DBLP:journals/corr/MousavianAFK16}, and DeepMANTA \cite{DBLP:journals/corr/ChabotCRTC17} it performs better than 3DOP \cite{Chen:2015:OPA:2969239.2969287} and Mono3D \cite{cvpr16chen}.  Therefore, our method achieves the start-of-the-art level performance for 3D orientation estimation. Besides, our approach consider both cars and vans from KITTI dataset, while the others are only evaluated on cars.

\tbd [OS = AOS / AP]

\begin{table}[H]
	\centering
	\caption{Comparison of the Orientation Score (OS) on official KITTI dataset for cars (ours is for cars and vans).}
	\label{os_cmp}
\begin{tabular}{|c|c|c|c|c|}
	\hline
	Method                                                                        & Type   & Easy           & Moderate       & Hard           \\ \hline
	3DOP \cite{Chen:2015:OPA:2969239.2969287}                    & Stereo & 98.28          & 97.13          & 96.73          \\ \hline
	Mono3D \cite{cvpr16chen}                                     & Mono   & 98.57          & 97.69          & 97.31          \\ \hline
	SubCNN \cite{DBLP:journals/corr/XiangCLS16}                  & Mono   & 99.84          & 99.52          & 99.25          \\ \hline
	Deep3DBox \cite{DBLP:journals/corr/MousavianAFK16}           & Mono   & 99.91 & 99.67          & 99.46          \\ \hline
	DeepMANTA (GoogLeNet) \cite{DBLP:journals/corr/ChabotCRTC17} & Mono   & 99.85          & \textbf{99.75} & \textbf{99.55} \\ \hline
	DeepMANTA (VGG16) \cite{DBLP:journals/corr/ChabotCRTC17}     & Mono   & 99.88          & 99.69          & 99.49          \\ \hline
	Ours (ResNet50)                                                               & Mono   & 99.88          & 98.78          & 97.39          \\ \hline
	Ours (VGG19)																	& Mono	 & \textbf{99.94}	       & 99.09	      & 98.43         \\ \hline
	
\end{tabular}
\end{table}

\subsubsection{3D dimension estimation, 2D part localization, and 2D part visibility}
Our approach is developed based on DeepMANTA \cite{DBLP:journals/corr/ChabotCRTC17}, so only DeepMANTA shares these three tasks with us. We present the results of DeepMANTA and our approach in Table \ref{the_rest_3}. And ours outperforms DeepMANTA on all these three tasks at all difficulty levels.

\begin{table}[H]
	\centering
	\caption{Comparison of 3D dimension estimation, 2D part localization, and 2D part visibility on official KITTI dataset for cars (ours is for cars and vans).}
	\label{the_rest_3}
	\resizebox{\textwidth}{!}{%
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
			\hline
			\multirow{2}{*}{Method} & \multirow{2}{*}{Type} & \multicolumn{3}{c|}{3D dimension estimation} & \multicolumn{3}{c|}{2D part localization} & \multicolumn{3}{c|}{2D part visibility} \\ \cline{3-11} 
			&  & Easy & Moderate & Hard & Easy & Moderate & Hard & Easy & Moderate & Hard \\ \hline
			DeepMANTA \cite{DBLP:journals/corr/ChabotCRTC17} & Mono & 97.54 & 90.79 & 82.64 & 92.48 & 85.08 & 76.9 & 94.04 & 86.62 & 78.72 \\ \hline
			Ours (ResNet50) & Mono & 99.76 & 99.6 & 99.48 & 99.32 & 98.9 & 97.52 & 97.77 & 94.92 & 93.92 \\ \hline
			Ours (VGG19) & Mono & \textbf{100} & \textbf{99.78} & \textbf{99.77} & \textbf{99.96} & \textbf{99.21} & \textbf{98.4} & \textbf{98.36} & \textbf{96.06} & \textbf{95.52} \\ \hline
		\end{tabular}
	}
\end{table}

\tbd [the methods used to compare on different tasks are inconsistent]
\tbd [better to show some images here to show the performance]