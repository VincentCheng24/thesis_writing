\subsection{Mono RGB Image Based Approaches}
Most vehicles are equipped with monocular cameras and RGB images have detailed texture information with high resolution so that plenty of approaches are developed based on monocular RGB images. Images are mainly contain 2D information so most approaches in this category require other helps. 

One way is to utilize the geometry information of the vehicles. In this category, a set of algorithms recover the 3D bounding box based on 3D car models. \cite{834, 741} model the 3D geometry representation of objects with 3D wireframe models. And the 3D bounding box is estimated based on the geometry constraints of the vehicle in image and its corresponding 3D wireframe model. 3DVP \cite{xiang2015data} performs 3D detection based on 3D Voxel Patterns which are generated from the KITTI dataset \cite{Geiger2012CVPR} and a set of 3D CAD models to encode the geometric information of objects. Mono3D \cite{cvpr16chen} introduces 3D proposals. It exhaustively places 3D bounding boxes on the ground-plane as proposals, then scores each proposal based on several hand-crafted geometric features, and applies a CNN to score the most promising candidates to generate the final 3D detections. Recently, CNNs are introduced to estimate some key features for 3D detection. Deep3DBox \cite{DBLP:journals/corr/MousavianAFK16} applies two CNNs to estimate the orientation and dimensions respectively, makes use of the geometric criterion that a 3D box should fit tightly within the 2D box of the vehicle to estimate the translation, and finally integrates them together to perform 3D detection. Deep MANTA \cite{DBLP:journals/corr/ChabotCRTC17} generates 3D detection based on 2D detection. It applies a CNN to predict some 2D key points and the template similarity of the vehicles in image and recovers the corresponding 3D key points and 3D dimensions with some 3D CAD vehicle models, and finally perform 3D detection via 2D-3D matching. Our approach is similar to Deep MANTA in spirit.

Another way is to use temporal information. \cite{7780838, 7298997} make use of motion structure and ground estimation to perform 3D detection from 2D bounding box.


These approaches are sensitive to the assumptions made and the parameter estimation accuracy so that they perform poorly on the 3D detection task compared to methods that use point cloud data \tbd.

\subsection{Stereo RGB Image Based Approaches}
A pair of stereo RGB images can provide the depth information of the current scene. 3DOP \cite{DBLP:journals/corr/ChenKZMFU16} recovers depth from stereo images and generates 3D box proposals with the constraints from depth and other geometry characters, which are forwarded to modified Fast R-CNN \cite{DBLP:journals/corr/Girshick15} pipeline for object detection and pose estimation. \cite{PHAM2017110} extends 3DOP by applying a separate CNN to extract features from depth and integrating them together for the final 3D detection.


\subsection{Point Cloud Based Approaches}

A LiDAR point cloud is a collection of 3D points acquired by a LiDAR laser scanner. It can represent 3D information of the surroundings but its resolution is lower than images'.

One set of approaches apply a point cloud by converting it into a 2D array and making use of the image-based methods to perform 3D detection tasks. This can alleviate the inherent problem that LiDAR point is often sparse and irregular in 3D voxel representation . VeloFCN \cite{DBLP:journals/corr/LiZX16} projects the point cloud to the front view to generate a 2D point map and then applies a fully convolutional neural network to estimate the 3D bounding boxes for vehicles on this 2D point map. \cite{DBLP:journals/corr/abs-1710-07368, Chen:2015:OPA:2969239.2969287} also fall into this category. Besides, the pre-trained models based on RGB images can be used to initialize the CNNs, which has been proven to be beneficial \cite{DBLP:journals/corr/GuptaGAM14}. However, the 2D representation of LiDAR cloud points suffers from object size variations because of its distance to the sensor and object overlapping.

Another category of approaches transforms the cloud point into a 3D voxel grid representation. Various quantities are used to encode voxels \cite{10.1007/978-3-319-10599-4_41, 7780463, DBLP:journals/corr/Li16p, Wang-RSS-15, DBLP:journals/corr/EngelckeRWTP16}, For example, in \cite{Wang-RSS-15, DBLP:journals/corr/EngelckeRWTP16}, the information in one non-empty cell is encoded with six quantities while empty cells contain no information. For the single-stage detectors, \eg Sliding Shapes \cite{10.1007/978-3-319-10599-4_41} and Vote3D \cite{Wang-RSS-15}, slide window across the 3D voxel space and apply SVM classifiers to perform 3D detection. To improve the performance, Vote3Deep \cite{DBLP:journals/corr/EngelckeRWTP16} uses a voting strategy with sparse 3D convolution. And \cite{DBLP:journals/corr/Li16p} feeds the point cloud voxel directly to a 3D FCN to generate 3D bounding boxes. For two-stage framework, \eg Voxelnet,  extends the 2D RPN \cite{DBLP:journals/corr/RenHG015} to 3D to generate 3D proposals and  apply a refine network to score these proposals \cite{DBLP:journals/corr/abs-1711-06396}. 3D CNNs boosts the 3D detection performance while the computation is very expensive and sparsity is to be explored.  


\subsection{Sensor Fusion Based Approaches}

Images are of high resolution but lack depth, while LiDAR data has 3D information but are sparse and irregular. Therefore many works have investigated combining these complementary data sources together to build robust 3D object detectors. 

One investigated direction is to extract information from these two sources separately for sub-functions.
Frustum PointNets \cite{DBLP:journals/corr/abs-1711-08488} first generates frustum point cloud proposals based on 2D object detection, then performs 3D instance segmentation in each frustum point cloud and finally applies a box estimation net to estimate the final 3D bounding box for each object. Similarly, \cite{DBLP:journals/corr/abs-1803-00387} first generates 2D bounding boxes and estimate vehicle dimensions for the target vehicles on images. Secondly, a model fitting algorithm estimates the 3D bounding box on a subsets of the point clouds which fall into the 2D bounding box after projection. Finally a refine 2D CNN performs the final 3D box regression and classification.



The other is to fuse these two kinds of data and process it as a whole. There are various ways to fuse data. One intuitive way is projection, \eg \cite{DBLP:journals/corr/EitelSSRB15} converts the point cloud to a dense depth image and then appends it as an additional channel of image, \cite{7487370} extends \cite{DBLP:journals/corr/EitelSSRB15} via converting the cloud point to a three-channel HHA map. Their 3D detection score is low mostly due to the lose of 3D information during projection. The more advanced way is to fuse their feature maps. MV3D \cite{DBLP:journals/corr/ChenMWLX16} first generates 3D proposals in LiDAR's bird's eye view and projects these proposals to the features maps of the image and the bird’s eye view and front view of LIDAR data. Then a deep network fuses three ROI pooling regions and performs 3D detection based on the fused features. PointFusion\cite{DBLP:journals/corr/abs-1711-10871} applies a ResNet \cite{DBLP:journals/corr/HeZRS15} to extract appearance and geometry features from image crops defined by the 2D bounding boxes and a modified PointNet \cite{DBLP:journals/corr/QiSMG16} to process the raw point cloud simultaneously, and finally uses a novel fusion network to integrate both features and estimate 3D bounding box. Recently, Zining \etal \cite{DBLP:journals/corr/abs-1711-06703} applies an innovative sparse non-homogeneous pooling layer to fuse the features extracted from bird’s eye view of LiDAR data and front view camera images by two separate CNNs before region proposal stage. And then a single-stage detector adapted from \cite{DBLP:journals/corr/abs-1708-02002} is designed to perform 3D detection on these fused data without RoI pooling, which is 6 times faster then MV3D. AVOD \cite{DBLP:journals/corr/abs-1712-02294} applies double feature fusions.  It uses two identical CNN to extract features from images and LiDAR data respectively and exploits a Multimodal Fusion Region Proposal Network to generate vehicle proposals from the fused feature crops, projection regions of anchors in the feature maps. Finally it applies a Multiview Detection Network to perform 3D detection on the fused feature crops corresponding to the proposals. It achieves the best performance on KITTI 3D vehicle detection up till now.




